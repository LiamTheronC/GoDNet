{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to figure out M6 post and to_world.\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from fractions import gcd\n",
    "from numbers import Number\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from utils import collate_fn,gpu,to_long\n",
    "import logging\n",
    "from memory_profiler import profile\n",
    "import gc\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "seed = 33\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "config = dict()\n",
    "config['n_actornet'] = 128\n",
    "config['num_epochs'] = 50\n",
    "config['lr'] = 1e-2\n",
    "config['train_split'] = '/home/avt/prediction/Waymo/data_processed/train'\n",
    "config['val_split'] = '/home/avt/prediction/Waymo/data_processed/validation'\n",
    "config[\"num_scales\"] = 6\n",
    "config[\"n_map\"] = 128\n",
    "config[\"n_actor\"] = 128\n",
    "config[\"actor2map_dist\"] = 7.0\n",
    "config[\"map2actor_dist\"] = 6.0\n",
    "config[\"actor2actor_dist\"] = 100.0\n",
    "config[\"num_mods\"] = 6\n",
    "config[\"pred_size\"] = 80\n",
    "config[\"pred_step\"] = 1\n",
    "config[\"num_preds\"] = config[\"pred_size\"] // config[\"pred_step\"]\n",
    "config['metrics'] = [30,50,80]\n",
    "config[\"cls_th\"] = 2.0 #5.0\n",
    "config[\"cls_ignore\"] = 0.2\n",
    "config[\"mgn\"] = 0.2\n",
    "config[\"cls_coef\"] = 1.0\n",
    "config[\"reg_coef\"] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out, norm='GN', ng=32, act=True):\n",
    "        super(Linear, self).__init__()\n",
    "        assert(norm in ['GN', 'BN', 'SyncBN'])\n",
    "\n",
    "        self.linear = nn.Linear(n_in, n_out, bias=False)\n",
    "        \n",
    "        if norm == 'GN':\n",
    "            self.norm = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
    "        elif norm == 'BN':\n",
    "            self.norm = nn.BatchNorm1d(n_out)\n",
    "        else:\n",
    "            exit('SyncBN has not been added!')\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.norm(out)\n",
    "        if self.act:\n",
    "            out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LinearRes(nn.Module):\n",
    "    def __init__(self, n_in, n_out, norm='GN', ng=32):\n",
    "        super(LinearRes, self).__init__()\n",
    "        assert(norm in ['GN', 'BN', 'SyncBN'])\n",
    "\n",
    "        self.linear1 = nn.Linear(n_in, n_out, bias=False)\n",
    "        self.linear2 = nn.Linear(n_out, n_out, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if norm == 'GN':\n",
    "            self.norm1 = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
    "            self.norm2 = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
    "        elif norm == 'BN':\n",
    "            self.norm1 = nn.BatchNorm1d(n_out)\n",
    "            self.norm2 = nn.BatchNorm1d(n_out)\n",
    "        else:   \n",
    "            exit('SyncBN has not been added!')\n",
    "\n",
    "        if n_in != n_out:\n",
    "            if norm == 'GN':\n",
    "                self.transform = nn.Sequential(\n",
    "                    nn.Linear(n_in, n_out, bias=False),\n",
    "                    nn.GroupNorm(gcd(ng, n_out), n_out))\n",
    "            elif norm == 'BN':\n",
    "                self.transform = nn.Sequential(\n",
    "                    nn.Linear(n_in, n_out, bias=False),\n",
    "                    nn.BatchNorm1d(n_out))\n",
    "            else:\n",
    "                exit('SyncBN has not been added!')\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.norm2(out)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            out += self.transform(x)\n",
    "            \n",
    "        else:\n",
    "            out += x\n",
    "        out = self.relu(out) \n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv1d(nn.Module):\n",
    "    def __init__(self, n_in, n_out, kernel_size=3, stride=1, norm='GN', ng=32, act=True):\n",
    "        super(Conv1d, self).__init__()\n",
    "        assert(norm in ['GN', 'BN', 'SyncBN'])\n",
    "\n",
    "        self.conv = nn.Conv1d(n_in, n_out, kernel_size=kernel_size, padding=(int(kernel_size) - 1) // 2, stride=stride, bias=False)\n",
    "\n",
    "        if norm == 'GN':\n",
    "            self.norm = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
    "        elif norm == 'BN':\n",
    "            self.norm = nn.BatchNorm1d(n_out)\n",
    "        else:\n",
    "            exit('SyncBN has not been added!')\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.norm(out)\n",
    "        if self.act:\n",
    "            out = self.relu(out)\n",
    "        return out\n",
    " \n",
    "\n",
    "class Res1d(nn.Module):\n",
    "    def __init__(self, n_in, n_out, kernel_size=3, stride=1, norm='GN', ng=32, act=True):\n",
    "        super(Res1d, self).__init__()\n",
    "        assert(norm in ['GN', 'BN', 'SyncBN'])\n",
    "        padding = (int(kernel_size) - 1) // 2\n",
    "        self.conv1 = nn.Conv1d(n_in, n_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.conv2 = nn.Conv1d(n_out, n_out, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "\n",
    "        # All use name bn1 and bn2 to load imagenet pretrained weights\n",
    "        if norm == 'GN':\n",
    "            self.bn1 = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
    "            self.bn2 = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
    "        elif norm == 'BN':\n",
    "            self.bn1 = nn.BatchNorm1d(n_out)\n",
    "            self.bn2 = nn.BatchNorm1d(n_out)\n",
    "        else:\n",
    "            exit('SyncBN has not been added!')\n",
    "\n",
    "        if stride != 1 or n_out != n_in:\n",
    "            if norm == 'GN':\n",
    "                self.downsample = nn.Sequential(\n",
    "                        nn.Conv1d(n_in, n_out, kernel_size=1, stride=stride, bias=False),\n",
    "                        nn.GroupNorm(gcd(ng, n_out), n_out))\n",
    "            elif norm == 'BN':\n",
    "                self.downsample = nn.Sequential(\n",
    "                        nn.Conv1d(n_in, n_out, kernel_size=1, stride=stride, bias=False),\n",
    "                        nn.BatchNorm1d(n_out))\n",
    "            else:\n",
    "                exit('SyncBN has not been added!')\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        out += x\n",
    "        if self.act:\n",
    "            out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def actor_gather(actors: List[Tensor]) -> Tuple[Tensor, List[Tensor]]:\n",
    "    \"\"\"\n",
    "    actors is data['feat']\n",
    "     \n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(actors)\n",
    "    num_actors = [len(x) for x in actors]\n",
    "\n",
    "    actors = [torch.stack(x).transpose(1, 2) for x in actors]\n",
    "    actors = torch.cat(actors, 0)\n",
    "\n",
    "    actor_idcs = []\n",
    "    count = 0\n",
    "    for i in range(batch_size):\n",
    "        idcs = torch.arange(count, count + num_actors[i]).to(actors.device)\n",
    "        actor_idcs.append(idcs)\n",
    "        count += num_actors[i]\n",
    "    return actors, actor_idcs\n",
    "\n",
    "\n",
    "def graph_gather(graphs):\n",
    "    batch_size = len(graphs)\n",
    "    node_idcs = []\n",
    "    count = 0\n",
    "    counts = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        counts.append(count)\n",
    "        idcs = torch.arange(count, count + graphs[i][\"num_nodes\"])\n",
    "        node_idcs.append(idcs)\n",
    "        count = count + graphs[i][\"num_nodes\"]\n",
    "\n",
    "    graph = dict()\n",
    "    graph[\"idcs\"] = node_idcs\n",
    "    graph[\"ctrs\"] = [x[\"ctrs\"] for x in graphs]\n",
    "\n",
    "    graph['feats'] = torch.cat([x['feats'] for x in graphs], 0)\n",
    "\n",
    "    for k1 in [\"pre\", \"suc\"]:\n",
    "        graph[k1] = []\n",
    "        for i in range(len(graphs[0][\"pre\"])):\n",
    "            graph[k1].append(dict())\n",
    "            for k2 in [\"u\", \"v\"]:\n",
    "                graph[k1][i][k2] = torch.cat(\n",
    "                    [graphs[j][k1][i][k2] + counts[j] for j in range(batch_size)], 0\n",
    "                )\n",
    "\n",
    "    for k1 in [\"left\", \"right\"]:\n",
    "        graph[k1] = dict()\n",
    "        for k2 in [\"u\", \"v\"]:\n",
    "            temp = [graphs[i][k1][k2] + counts[i] for i in range(batch_size)]\n",
    "            temp = [\n",
    "                x if x.dim() > 0 else graph[\"pre\"][0][\"u\"].new().resize_(0)\n",
    "                for x in temp\n",
    "            ]\n",
    "            graph[k1][k2] = torch.cat(temp)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "class ActorNet(nn.Module):\n",
    "    def __init__(self,config) -> None:\n",
    "        super(ActorNet,self).__init__()\n",
    "        self.config = config\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "\n",
    "        n_in = 4\n",
    "        n_out = [32, 64, 128]\n",
    "        blocks = [Res1d, Res1d, Res1d]\n",
    "        num_blocks = [2, 2, 2]\n",
    "\n",
    "        groups = []\n",
    "\n",
    "        for i in range(len(num_blocks)):\n",
    "\n",
    "            group = []\n",
    "\n",
    "            if i == 0:\n",
    "                group.append(blocks[i](n_in, n_out[i], norm=norm, ng=ng))\n",
    "            else:\n",
    "                group.append(blocks[i](n_in, n_out[i], stride=2, norm=norm, ng=ng))\n",
    "\n",
    "            for j in range(1, num_blocks[i]):\n",
    "                group.append(blocks[i](n_out[i], n_out[i], norm=norm, ng=ng))\n",
    "            \n",
    "            groups.append(nn.Sequential(*group))\n",
    "            \n",
    "            n_in = n_out[i]\n",
    "\n",
    "        self.groups = nn.ModuleList(groups)\n",
    "\n",
    "        n = config['n_actornet']#128\n",
    "        \n",
    "        lateral = []\n",
    "        for i in range(len(n_out)):\n",
    "            lateral.append(Conv1d(n_out[i], n, norm=norm, ng=ng, act=False))\n",
    "        self.lateral = nn.ModuleList(lateral)\n",
    "\n",
    "        self.outlayer = Res1d(n, n, norm=norm, ng=ng)\n",
    "\n",
    "    def forward(self, actors: Tensor) -> Tensor:\n",
    "        #actors [batch_size,feature_dim(4),time_step(11)]\n",
    "        \n",
    "        out = actors\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(len(self.groups)):\n",
    "            out = self.groups[i](out)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        out = self.lateral[-1](outputs[-1])\n",
    "\n",
    "        for i in range(len(outputs) - 2, -1, -1):\n",
    "\n",
    "            out = F.interpolate(out, scale_factor=2, mode=\"linear\", align_corners=False)\n",
    "            tmp = self.lateral[i](outputs[i])\n",
    "\n",
    "            if out.shape != tmp.shape:\n",
    "                out = out[:,:,:tmp.shape[2]]\n",
    "\n",
    "            out += tmp\n",
    "        \n",
    "        out = self.outlayer(out)[:,:,-1]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MapNet(nn.Module):\n",
    "  \n",
    "    def __init__(self, config):\n",
    "        super(MapNet, self).__init__()\n",
    "        self.config = config\n",
    "        n_map = 128\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "\n",
    "        self.input = nn.Sequential(\n",
    "            nn.Linear(3, n_map),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Linear(n_map, n_map, norm=norm, ng=ng, act=False),\n",
    "        )\n",
    "        self.seg = nn.Sequential(\n",
    "            nn.Linear(3, n_map),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Linear(n_map, n_map, norm=norm, ng=ng, act=False),\n",
    "        )\n",
    "\n",
    "        keys = [\"ctr\", \"norm\", \"ctr2\", \"left\", \"right\"]\n",
    "        for i in range(config[\"num_scales\"]):\n",
    "            keys.append(\"pre\" + str(i))\n",
    "            keys.append(\"suc\" + str(i))\n",
    "\n",
    "        fuse = dict()\n",
    "        for key in keys:\n",
    "            fuse[key] = []\n",
    "\n",
    "        for i in range(4):\n",
    "            for key in fuse:\n",
    "                if key in [\"norm\"]:\n",
    "                    fuse[key].append(nn.GroupNorm(gcd(ng, n_map), n_map))\n",
    "                elif key in [\"ctr2\"]:\n",
    "                    fuse[key].append(Linear(n_map, n_map, norm=norm, ng=ng, act=False))\n",
    "                else:\n",
    "                    fuse[key].append(nn.Linear(n_map, n_map, bias=False))\n",
    "\n",
    "        for key in fuse:\n",
    "            fuse[key] = nn.ModuleList(fuse[key])\n",
    "        self.fuse = nn.ModuleDict(fuse)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        if (\n",
    "            len(graph[\"feats\"]) == 0\n",
    "            or len(graph[\"pre\"][-1][\"u\"]) == 0\n",
    "            or len(graph[\"suc\"][-1][\"u\"]) == 0\n",
    "        ):\n",
    "            temp = graph[\"feats\"]\n",
    "            return (\n",
    "                temp.new().resize_(0),\n",
    "                [temp.new().long().resize_(0) for x in graph[\"node_idcs\"]],\n",
    "                temp.new().resize_(0),\n",
    "            )\n",
    "\n",
    "        ctrs = torch.cat(graph[\"ctrs\"], 0)\n",
    "        feat = self.input(ctrs)\n",
    "        feat += self.seg(graph[\"feats\"])\n",
    "        feat = self.relu(feat)\n",
    "\n",
    "        \"\"\"fuse map\"\"\"\n",
    "        res = feat\n",
    "        for i in range(len(self.fuse[\"ctr\"])):\n",
    "            temp = self.fuse[\"ctr\"][i](feat)\n",
    "            for key in self.fuse:\n",
    "                if key.startswith(\"pre\") or key.startswith(\"suc\"):\n",
    "                    k1 = key[:3]\n",
    "                    k2 = int(key[3:])\n",
    "                    temp.index_add_(\n",
    "                        0,\n",
    "                        graph[k1][k2][\"u\"],\n",
    "                        self.fuse[key][i](feat[graph[k1][k2][\"v\"]]),\n",
    "                    )\n",
    "\n",
    "            if len(graph[\"left\"][\"u\"] > 0):\n",
    "                temp.index_add_(\n",
    "                    0,\n",
    "                    graph[\"left\"][\"u\"],\n",
    "                    self.fuse[\"left\"][i](feat[graph[\"left\"][\"v\"]]),\n",
    "                )\n",
    "            if len(graph[\"right\"][\"u\"] > 0):\n",
    "                temp.index_add_(\n",
    "                    0,\n",
    "                    graph[\"right\"][\"u\"],\n",
    "                    self.fuse[\"right\"][i](feat[graph[\"right\"][\"v\"]]),\n",
    "                )\n",
    "\n",
    "            feat = self.fuse[\"norm\"][i](temp)\n",
    "            feat = self.relu(feat)\n",
    "\n",
    "            feat = self.fuse[\"ctr2\"][i](feat)\n",
    "            feat += res\n",
    "            feat = self.relu(feat)\n",
    "            res = feat\n",
    "\n",
    "        return feat , graph[\"idcs\"], graph[\"ctrs\"]\n",
    "\n",
    "\n",
    "class Att(nn.Module):\n",
    "    def __init__(self, n_agt: int, n_ctx: int) -> None:\n",
    "        super(Att, self).__init__()\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "\n",
    "        self.dist = nn.Sequential(\n",
    "            nn.Linear(3, n_ctx),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Linear(n_ctx, n_ctx, norm=norm, ng=ng),\n",
    "        )\n",
    "\n",
    "        self.query = Linear(n_agt, n_ctx, norm=norm, ng=ng)\n",
    "\n",
    "        self.ctx = nn.Sequential(\n",
    "            Linear(3 * n_ctx, n_agt, norm=norm, ng=ng),\n",
    "            nn.Linear(n_agt, n_agt, bias=False),\n",
    "        )\n",
    "\n",
    "        self.agt = nn.Linear(n_agt, n_agt, bias=False)\n",
    "        self.norm = nn.GroupNorm(gcd(ng, n_agt), n_agt)\n",
    "        self.linear = Linear(n_agt, n_agt, norm=norm, ng=ng, act=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, agts: Tensor, agt_idcs: List[Tensor], agt_ctrs: List[Tensor], ctx: Tensor, ctx_idcs: List[Tensor], ctx_ctrs: List[Tensor], dist_th: float) -> Tensor:\n",
    "        # feat, graph[\"idcs\"], graph[\"ctrs\"], actors, actor_idcs, actor_ctrs, config[\"actor2map_dist\"]      \n",
    "        res = agts\n",
    "        if len(ctx) == 0:\n",
    "            agts = self.agt(agts)\n",
    "            agts = self.relu(agts)\n",
    "            agts = self.linear(agts)\n",
    "            agts += res\n",
    "            agts = self.relu(agts)\n",
    "            return agts\n",
    "\n",
    "        batch_size = len(agt_idcs)\n",
    "        hi, wi = [], []\n",
    "        hi_count, wi_count = 0, 0\n",
    "\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "        \n",
    "            dist = agt_ctrs[i].view(-1, 1, 3) - ctx_ctrs[i].view(1, -1, 3)\n",
    "            dist = torch.sqrt((dist ** 2).sum(2))\n",
    "            mask = dist <= dist_th\n",
    "\n",
    "            idcs = torch.nonzero(mask, as_tuple=False)\n",
    "            if len(idcs) == 0:\n",
    "                continue\n",
    "\n",
    "            hi.append(idcs[:, 0] + hi_count)\n",
    "            wi.append(idcs[:, 1] + wi_count)\n",
    "            hi_count += len(agt_idcs[i])\n",
    "            wi_count += len(ctx_idcs[i])\n",
    "\n",
    "        if hi == []:\n",
    "            print('WARNING!!! - Attention')\n",
    "\n",
    "        hi = torch.cat(hi, 0)\n",
    "        wi = torch.cat(wi, 0)\n",
    "\n",
    "        agt_ctrs = torch.cat(agt_ctrs, 0)\n",
    "        ctx_ctrs = torch.cat(ctx_ctrs, 0)\n",
    "        dist = agt_ctrs[hi] - ctx_ctrs[wi]\n",
    "        dist = self.dist(dist)\n",
    "\n",
    "        query = self.query(agts[hi])\n",
    "\n",
    "        ctx = ctx[wi]\n",
    "        ctx = torch.cat((dist, query, ctx), 1)\n",
    "        ctx = self.ctx(ctx)\n",
    "\n",
    "        agts = self.agt(agts)\n",
    "        agts.index_add_(0, hi, ctx)\n",
    "        agts = self.norm(agts)\n",
    "        agts = self.relu(agts)\n",
    "\n",
    "        agts = self.linear(agts)\n",
    "        agts += res\n",
    "        agts = self.relu(agts)\n",
    "\n",
    "        return agts\n",
    "\n",
    "\n",
    "class A2M(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor to Map Fusion:  fuses real-time traffic information from\n",
    "    actor nodes to lane nodes\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(A2M, self).__init__()\n",
    "        self.config = config\n",
    "        n_map = config[\"n_map\"]\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "\n",
    "        \"\"\"fuse meta, static, dyn\"\"\"\n",
    "        self.meta = Linear(n_map, n_map, norm=norm, ng=ng)\n",
    "        att = []\n",
    "        for i in range(2):\n",
    "            att.append(Att(n_map, config[\"n_actor\"]))\n",
    "        self.att = nn.ModuleList(att)\n",
    "\n",
    "    def forward(self, feat: Tensor, graph: Dict[str, Union[List[Tensor], Tensor, List[Dict[str, Tensor]], Dict[str, Tensor]]], actors: Tensor, actor_idcs: List[Tensor], actor_ctrs: List[Tensor]) -> Tensor:\n",
    "        \"\"\"meta, static and dyn fuse using attention\"\"\"\n",
    "        \n",
    "        feat = self.meta(feat)\n",
    "\n",
    "        for i in range(len(self.att)):\n",
    "            feat = self.att[i](\n",
    "                feat,\n",
    "                graph[\"idcs\"],\n",
    "                graph[\"ctrs\"],\n",
    "                actors,\n",
    "                actor_idcs,\n",
    "                actor_ctrs,\n",
    "                self.config[\"actor2map_dist\"],\n",
    "            )\n",
    "        return feat\n",
    "\n",
    "\n",
    "class M2M(nn.Module):\n",
    " \n",
    "    def __init__(self, config):\n",
    "        super(M2M, self).__init__()\n",
    "        self.config = config\n",
    "        n_map = config[\"n_map\"]\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "\n",
    "        keys = [\"ctr\", \"norm\", \"ctr2\", \"left\", \"right\"]\n",
    "        for i in range(config[\"num_scales\"]):\n",
    "            keys.append(\"pre\" + str(i))\n",
    "            keys.append(\"suc\" + str(i))\n",
    "\n",
    "        fuse = dict()\n",
    "        for key in keys:\n",
    "            fuse[key] = []\n",
    "\n",
    "        for i in range(4):\n",
    "            for key in fuse:\n",
    "                if key in [\"norm\"]:\n",
    "                    fuse[key].append(nn.GroupNorm(gcd(ng, n_map), n_map))\n",
    "                elif key in [\"ctr2\"]:\n",
    "                    fuse[key].append(Linear(n_map, n_map, norm=norm, ng=ng, act=False))\n",
    "                else:\n",
    "                    fuse[key].append(nn.Linear(n_map, n_map, bias=False))\n",
    "\n",
    "        for key in fuse:\n",
    "            fuse[key] = nn.ModuleList(fuse[key])\n",
    "        self.fuse = nn.ModuleDict(fuse)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, feat: Tensor, graph: Dict) -> Tensor:\n",
    "        \"\"\"fuse map\"\"\"\n",
    "        res = feat\n",
    "        for i in range(len(self.fuse[\"ctr\"])):\n",
    "            temp = self.fuse[\"ctr\"][i](feat)\n",
    "            for key in self.fuse:\n",
    "                if key.startswith(\"pre\") or key.startswith(\"suc\"):\n",
    "                    k1 = key[:3]\n",
    "                    k2 = int(key[3:])\n",
    "                    temp.index_add_(\n",
    "                        0,\n",
    "                        graph[k1][k2][\"u\"],\n",
    "                        self.fuse[key][i](feat[graph[k1][k2][\"v\"]]),\n",
    "                    )\n",
    "\n",
    "            if len(graph[\"left\"][\"u\"] > 0):\n",
    "                temp.index_add_(\n",
    "                    0,\n",
    "                    graph[\"left\"][\"u\"],\n",
    "                    self.fuse[\"left\"][i](feat[graph[\"left\"][\"v\"]]),\n",
    "                )\n",
    "            if len(graph[\"right\"][\"u\"] > 0):\n",
    "                temp.index_add_(\n",
    "                    0,\n",
    "                    graph[\"right\"][\"u\"],\n",
    "                    self.fuse[\"right\"][i](feat[graph[\"right\"][\"v\"]]),\n",
    "                )\n",
    "\n",
    "            feat = self.fuse[\"norm\"][i](temp)\n",
    "            feat = self.relu(feat)\n",
    "\n",
    "            feat = self.fuse[\"ctr2\"][i](feat)\n",
    "            feat += res\n",
    "            feat = self.relu(feat)\n",
    "            res = feat\n",
    "        return feat\n",
    "\n",
    "\n",
    "class M2A(nn.Module):\n",
    "    \"\"\"\n",
    "    The lane to actor block fuses updated\n",
    "        map information from lane nodes to actor nodes\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(M2A, self).__init__()\n",
    "        self.config = config\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "\n",
    "        n_actor = config[\"n_actor\"]\n",
    "        n_map = config[\"n_map\"]\n",
    "\n",
    "        att = []\n",
    "        for i in range(2):\n",
    "            att.append(Att(n_actor, n_map))\n",
    "        self.att = nn.ModuleList(att)\n",
    "\n",
    "    def forward(self, actors: Tensor, actor_idcs: List[Tensor], actor_ctrs: List[Tensor], nodes: Tensor, node_idcs: List[Tensor], node_ctrs: List[Tensor]) -> Tensor:\n",
    "        for i in range(len(self.att)):\n",
    "            actors = self.att[i](\n",
    "                actors,\n",
    "                actor_idcs,\n",
    "                actor_ctrs,\n",
    "                nodes,\n",
    "                node_idcs,\n",
    "                node_ctrs,\n",
    "                self.config[\"map2actor_dist\"],\n",
    "            )\n",
    "        return actors\n",
    "\n",
    "\n",
    "class A2A(nn.Module):\n",
    "    \"\"\"\n",
    "    The actor to actor block performs interactions among actors.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(A2A, self).__init__()\n",
    "        self.config = config\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "\n",
    "        n_actor = config[\"n_actor\"]\n",
    "        n_map = config[\"n_map\"]\n",
    "\n",
    "        att = []\n",
    "        for i in range(2):\n",
    "            att.append(Att(n_actor, n_actor))\n",
    "        self.att = nn.ModuleList(att)\n",
    "\n",
    "    def forward(self, actors: Tensor, actor_idcs: List[Tensor], actor_ctrs: List[Tensor]) -> Tensor:\n",
    "        for i in range(len(self.att)):\n",
    "            actors = self.att[i](\n",
    "                actors,\n",
    "                actor_idcs,\n",
    "                actor_ctrs,\n",
    "                actors,\n",
    "                actor_idcs,\n",
    "                actor_ctrs,\n",
    "                self.config[\"actor2actor_dist\"],\n",
    "            )\n",
    "        return actors\n",
    "\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.config = config\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "\n",
    "        n_actor = config[\"n_actor\"]\n",
    "\n",
    "        pred = []\n",
    "        for i in range(config[\"num_mods\"]):\n",
    "            pred.append(\n",
    "                nn.Sequential(\n",
    "                    LinearRes(n_actor, n_actor, norm=norm, ng=ng),\n",
    "                    nn.Linear(n_actor, 2 * config[\"num_preds\"]),\n",
    "                )\n",
    "            )\n",
    "        self.pred = nn.ModuleList(pred)\n",
    "\n",
    "        self.att_dest = AttDest(n_actor)\n",
    "        self.cls = nn.Sequential(\n",
    "            LinearRes(n_actor, n_actor, norm=norm, ng=ng), nn.Linear(n_actor, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, actors: Tensor, actor_idcs: List[Tensor], actor_ctrs: List[Tensor]) -> Dict[str, List[Tensor]]:\n",
    "        preds = []\n",
    "        for i in range(len(self.pred)):\n",
    "            preds.append(self.pred[i](actors))\n",
    "        reg = torch.cat([x.unsqueeze(1) for x in preds], 1)\n",
    "        reg = reg.view(reg.size(0), reg.size(1), -1, 2)\n",
    "\n",
    "        for i in range(len(actor_idcs)):\n",
    "            idcs = actor_idcs[i]\n",
    "            ctrs = actor_ctrs[i].view(-1, 1, 1, 3)\n",
    "            reg[idcs] = reg[idcs] + ctrs[:,:,:,:2]\n",
    "\n",
    "        dest_ctrs = reg[:, :, -1].detach()\n",
    "        feats = self.att_dest(actors, torch.cat(actor_ctrs, 0), dest_ctrs)\n",
    "        cls = self.cls(feats).view(-1, self.config[\"num_mods\"])\n",
    "\n",
    "        cls, sort_idcs = cls.sort(1, descending=True)\n",
    "        row_idcs = torch.arange(len(sort_idcs)).long().to(sort_idcs.device)\n",
    "        row_idcs = row_idcs.view(-1, 1).repeat(1, sort_idcs.size(1)).view(-1)\n",
    "        sort_idcs = sort_idcs.view(-1)\n",
    "        reg = reg[row_idcs, sort_idcs].view(cls.size(0), cls.size(1), -1, 2)\n",
    "\n",
    "        out = dict()\n",
    "        out[\"cls\"], out[\"reg\"] = [], []\n",
    "        for i in range(len(actor_idcs)):\n",
    "            idcs = actor_idcs[i]\n",
    "            out[\"cls\"].append(cls[idcs])\n",
    "            out[\"reg\"].append(reg[idcs])\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttDest(nn.Module):\n",
    "    def __init__(self, n_agt: int):\n",
    "        super(AttDest, self).__init__()\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "\n",
    "        self.dist = nn.Sequential(\n",
    "            nn.Linear(2, n_agt),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Linear(n_agt, n_agt, norm=norm, ng=ng),\n",
    "        )\n",
    "\n",
    "        self.agt = Linear(2 * n_agt, n_agt, norm=norm, ng=ng)\n",
    "\n",
    "    def forward(self, agts: Tensor, agt_ctrs: Tensor, dest_ctrs: Tensor) -> Tensor:\n",
    "        n_agt = agts.size(1)\n",
    "        num_mods = dest_ctrs.size(1)\n",
    "\n",
    "        dist = (agt_ctrs[:,:2].unsqueeze(1) - dest_ctrs).view(-1, 2)\n",
    "        dist = self.dist(dist)\n",
    "        agts = agts.unsqueeze(1).repeat(1, num_mods, 1).view(-1, n_agt)\n",
    "\n",
    "        agts = torch.cat((dist, agts), 1)\n",
    "        agts = self.agt(agts)\n",
    "        return agts\n",
    "\n",
    "\n",
    "class GreatNet(nn.Module):\n",
    "    def __init__(self,config) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.actor_net = ActorNet(config)\n",
    "        self.map_net = MapNet(config)\n",
    "\n",
    "        self.a2m = A2M(config)\n",
    "        self.m2m = M2M(config)\n",
    "        self.m2a = M2A(config)\n",
    "        self.a2a = A2A(config)\n",
    "        \n",
    "        self.pred_net = PredNet(config)\n",
    "    \n",
    "    def forward(self, data: Dict) -> Tensor:\n",
    "\n",
    "        actors, actor_idcs = actor_gather(data[\"feats\"])\n",
    "        actor_ctrs = [torch.stack(i,0) for i in data[\"ctrs\"]]\n",
    "\n",
    "        actors = gpu(actors)\n",
    "        actor_idcs = gpu(actor_idcs)\n",
    "        actor_ctrs = gpu(actor_ctrs)\n",
    "\n",
    "        actors = self.actor_net(actors)\n",
    "\n",
    "        #------------------------------------------------------------#\n",
    "\n",
    "        graph = to_long(data['graph'])\n",
    "        graph = graph_gather(graph)\n",
    "\n",
    "        graph = gpu(graph)\n",
    "\n",
    "        nodes, node_idcs, node_ctrs = self.map_net(graph)\n",
    "\n",
    "        #------------------------------------------------------------#\n",
    "        \n",
    "        nodes = self.a2m(nodes, graph, actors, actor_idcs, actor_ctrs)\n",
    "        nodes = self.m2m(nodes, graph)\n",
    "        actors = self.m2a(actors, actor_idcs, actor_ctrs, nodes, node_idcs, node_ctrs)\n",
    "        actors = self.a2a(actors, actor_idcs, actor_ctrs)\n",
    "        \n",
    "        out = self.pred_net(actors, actor_idcs, actor_ctrs)\n",
    "        rot, orig = gpu(data[\"rot\"]), gpu(data[\"orig\"])\n",
    "\n",
    "        # to_global\n",
    "        for i in range(len(out[\"reg\"])):\n",
    "            out[\"reg\"][i] = torch.matmul(out[\"reg\"][i], rot[i]) + orig[i][:2].view(1, 1, 1, -1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class PredLoss(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PredLoss, self).__init__()\n",
    "        self.config = config\n",
    "        self.reg_loss = nn.SmoothL1Loss(reduction=\"sum\")\n",
    "\n",
    "    def forward(self, out: Dict[str, List[Tensor]], data) -> Dict[str, Union[Tensor, int]]:\n",
    "        cls, reg = out[\"cls\"], out[\"reg\"]\n",
    "        cls = torch.cat([x for x in cls], 0)\n",
    "        reg = torch.cat([x for x in reg], 0)\n",
    "        has_preds = pre_gather(data['has_preds']).cuda()\n",
    "        gt_preds = pre_gather(data['gt2_preds']).float()[:,:,:2].cuda()\n",
    "\n",
    "        loss_out = dict()\n",
    "        zero = 0.0 * (cls.sum() + reg.sum())\n",
    "        loss_out[\"cls_loss\"] = zero.clone()\n",
    "        loss_out[\"num_cls\"] = 0\n",
    "        loss_out[\"reg_loss\"] = zero.clone()\n",
    "        loss_out[\"num_reg\"] = 0\n",
    "\n",
    "        num_mods, num_preds = self.config[\"num_mods\"], self.config[\"num_preds\"]\n",
    "        # assert(has_preds.all())\n",
    "\n",
    "        last = has_preds.float() + 0.1 * torch.arange(num_preds).float().to(\n",
    "            has_preds.device\n",
    "        ) / float(num_preds)\n",
    "        max_last, last_idcs = last.max(1)\n",
    "        mask = max_last > 1.0\n",
    "\n",
    "        cls = cls[mask]\n",
    "        reg = reg[mask]\n",
    "        gt_preds = gt_preds[mask]\n",
    "        has_preds = has_preds[mask]\n",
    "        last_idcs = last_idcs[mask]\n",
    "\n",
    "        row_idcs = torch.arange(len(last_idcs)).long().to(last_idcs.device)\n",
    "        dist = []\n",
    "        for j in range(num_mods):\n",
    "            dist.append(\n",
    "                torch.sqrt(\n",
    "                    (\n",
    "                        (reg[row_idcs, j, last_idcs] - gt_preds[row_idcs, last_idcs])\n",
    "                        ** 2\n",
    "                    ).sum(1)\n",
    "                )\n",
    "            )\n",
    "        dist = torch.cat([x.unsqueeze(1) for x in dist], 1)\n",
    "        min_dist, min_idcs = dist.min(1)\n",
    "        row_idcs = torch.arange(len(min_idcs)).long().to(min_idcs.device)\n",
    "\n",
    "        mgn = cls[row_idcs, min_idcs].unsqueeze(1) - cls\n",
    "        mask0 = (min_dist < self.config[\"cls_th\"]).view(-1, 1)\n",
    "        mask1 = dist - min_dist.view(-1, 1) > self.config[\"cls_ignore\"]\n",
    "        mgn = mgn[mask0 * mask1]\n",
    "        mask = mgn < self.config[\"mgn\"]\n",
    "        coef = self.config[\"cls_coef\"]\n",
    "        loss_out[\"cls_loss\"] += coef * (\n",
    "            self.config[\"mgn\"] * mask.sum() - mgn[mask].sum()\n",
    "        )\n",
    "        loss_out[\"num_cls\"] += mask.sum().item()\n",
    "\n",
    "        reg = reg[row_idcs, min_idcs]\n",
    "        coef = self.config[\"reg_coef\"]\n",
    "        loss_out[\"reg_loss\"] += coef * self.reg_loss(\n",
    "            reg[has_preds], gt_preds[has_preds]\n",
    "        )\n",
    "        loss_out[\"num_reg\"] += has_preds.sum().item()\n",
    "\n",
    "        return loss_out\n",
    "\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Loss, self).__init__()\n",
    "        self.config = config\n",
    "        self.pred_loss = PredLoss(config)\n",
    "\n",
    "    def forward(self, out: Dict, data: Dict) -> Dict:\n",
    "        loss_out = self.pred_loss(out, data)\n",
    "        loss_out[\"loss\"] = loss_out[\"cls_loss\"] / (\n",
    "            loss_out[\"num_cls\"] + 1e-10\n",
    "        ) + loss_out[\"reg_loss\"] / (loss_out[\"num_reg\"] + 1e-10)\n",
    "        return loss_out\n",
    "    \n",
    "\n",
    "\n",
    "class PostProcess(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PostProcess, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, out,data):\n",
    "        post_out = dict()\n",
    "        post_out[\"preds\"] = [x[0:1].detach().cpu().numpy() for x in out[\"reg\"]]\n",
    "        \n",
    "        post_out[\"gt_preds\"] = [x[0:1].numpy() for x in gather(data[\"gt_preds\"])]\n",
    "        post_out[\"has_preds\"] = [x[0:1].numpy() for x in gather(data[\"has_preds\"])]\n",
    "        return post_out\n",
    "\n",
    "    def append(self, metrics: Dict, loss_out: Dict, post_out: Optional[Dict[str, List[ndarray]]]=None) -> Dict:\n",
    "        if len(metrics.keys()) == 0:\n",
    "            for key in loss_out:\n",
    "                if key != \"loss\":\n",
    "                    metrics[key] = 0.0\n",
    "\n",
    "            for key in post_out:\n",
    "                metrics[key] = []\n",
    "\n",
    "        for key in loss_out:\n",
    "            if key == \"loss\":\n",
    "                continue\n",
    "            if isinstance(loss_out[key], torch.Tensor):\n",
    "                metrics[key] += loss_out[key].item()\n",
    "            else:\n",
    "                metrics[key] += loss_out[key]\n",
    "\n",
    "        for key in post_out:\n",
    "            metrics[key] += post_out[key]\n",
    "        return metrics\n",
    "\n",
    "    def display(self, metrics, dt, epoch, lr=None):\n",
    "        \"\"\"Every display-iters print training/val information\"\"\"\n",
    "        if lr is not None:\n",
    "            print(\"Epoch %3.3f, lr %.5f, time %3.2f\" % (epoch, lr, dt))\n",
    "        else:\n",
    "            print(\n",
    "                \"************************* Validation, time %3.2f *************************\"\n",
    "                % dt\n",
    "            )\n",
    "\n",
    "        cls = metrics[\"cls_loss\"] / (metrics[\"num_cls\"] + 1e-10)\n",
    "        reg = metrics[\"reg_loss\"] / (metrics[\"num_reg\"] + 1e-10)\n",
    "        loss = cls + reg\n",
    "\n",
    "        preds = np.concatenate(metrics[\"preds\"], 0)\n",
    "        gt_preds = np.concatenate(metrics[\"gt_preds\"], 0)\n",
    "        has_preds = np.concatenate(metrics[\"has_preds\"], 0)\n",
    "        ade1, fde1, ade, fde, min_idcs = pred_metrics(preds, gt_preds, has_preds)\n",
    "\n",
    "        print(\n",
    "            \"loss %2.4f %2.4f %2.4f, ade1 %2.4f, fde1 %2.4f, ade %2.4f, fde %2.4f\"\n",
    "            % (loss, cls, reg, ade1, fde1, ade, fde)\n",
    "        )\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "def pred_metrics(preds, gt_preds, has_preds):\n",
    "    assert has_preds.all()\n",
    "    preds = np.asarray(preds, np.float32)\n",
    "    gt_preds = np.asarray(gt_preds, np.float32)\n",
    "\n",
    "    \"\"\"batch_size x num_mods x num_preds\"\"\"\n",
    "    err = np.sqrt(((preds - np.expand_dims(gt_preds, 1)) ** 2).sum(3))\n",
    "\n",
    "    ade1 = err[:, 0].mean()\n",
    "    fde1 = err[:, 0, -1].mean()\n",
    "\n",
    "    min_idcs = err[:, :, -1].argmin(1)\n",
    "    row_idcs = np.arange(len(min_idcs)).astype(np.int64)\n",
    "    err = err[row_idcs, min_idcs]\n",
    "    ade = err.mean()\n",
    "    fde = err[:, -1].mean()\n",
    "    return ade1, fde1, ade, fde, min_idcs\n",
    "\n",
    "\n",
    "\n",
    "def pre_gather(gts: List) -> Tensor:\n",
    "    tmp = list()\n",
    "    for g in gts:\n",
    "        tmp += g\n",
    "    \n",
    "    tmp = torch.stack(tmp)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "def gather(gts) -> list:\n",
    "\n",
    "    tmp = list()\n",
    "    for i,g in enumerate(gts):\n",
    "        zz = torch.stack(g, dim=0)\n",
    "        tmp.append(zz)\n",
    "    \n",
    "    return tmp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class W_Dataset(Dataset):\n",
    "    def __init__(self,path) -> None:\n",
    "\n",
    "        self.path = path\n",
    "        self.files = os.listdir(path)\n",
    "    \n",
    "    def __getitem__(self, index) -> dict:\n",
    "\n",
    "        data_path = os.path.join(self.path,self.files[index])\n",
    "        data = torch.load(data_path)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dataset_train = W_Dataset(config['train_split'])\n",
    "train_loader = DataLoader(dataset_train, \n",
    "                        batch_size = batch_size ,\n",
    "                        collate_fn = collate_fn, \n",
    "                        shuffle = True, \n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_304061/2376440236.py:111: DeprecationWarning: fractions.gcd() is deprecated. Use math.gcd() instead.\n",
      "  self.bn1 = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
      "/tmp/ipykernel_304061/2376440236.py:112: DeprecationWarning: fractions.gcd() is deprecated. Use math.gcd() instead.\n",
      "  self.bn2 = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
      "/tmp/ipykernel_304061/2376440236.py:123: DeprecationWarning: fractions.gcd() is deprecated. Use math.gcd() instead.\n",
      "  nn.GroupNorm(gcd(ng, n_out), n_out))\n",
      "/tmp/ipykernel_304061/2376440236.py:83: DeprecationWarning: fractions.gcd() is deprecated. Use math.gcd() instead.\n",
      "  self.norm = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
      "/tmp/ipykernel_304061/2376440236.py:9: DeprecationWarning: fractions.gcd() is deprecated. Use math.gcd() instead.\n",
      "  self.norm = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
      "/tmp/ipykernel_304061/2376440236.py:312: DeprecationWarning: fractions.gcd() is deprecated. Use math.gcd() instead.\n",
      "  fuse[key].append(nn.GroupNorm(gcd(ng, n_map), n_map))\n",
      "/tmp/ipykernel_304061/2376440236.py:399: DeprecationWarning: fractions.gcd() is deprecated. Use math.gcd() instead.\n",
      "  self.norm = nn.GroupNorm(gcd(ng, n_agt), n_agt)\n",
      "/tmp/ipykernel_304061/2376440236.py:521: DeprecationWarning: fractions.gcd() is deprecated. Use math.gcd() instead.\n",
      "  fuse[key].append(nn.GroupNorm(gcd(ng, n_map), n_map))\n",
      "/tmp/ipykernel_304061/2376440236.py:36: DeprecationWarning: fractions.gcd() is deprecated. Use math.gcd() instead.\n",
      "  self.norm1 = nn.GroupNorm(gcd(ng, n_out), n_out)\n",
      "/tmp/ipykernel_304061/2376440236.py:37: DeprecationWarning: fractions.gcd() is deprecated. Use math.gcd() instead.\n",
      "  self.norm2 = nn.GroupNorm(gcd(ng, n_out), n_out)\n"
     ]
    }
   ],
   "source": [
    "net = GreatNet(config).cuda()\n",
    "loss_f = Loss(config).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "        if i > 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(data)\n",
    "loss_out = loss_f(out,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lastIdcs(out, num_pred, data):\n",
    "    cls, reg = out[\"cls\"], out[\"reg\"]\n",
    "    gt_preds, has_preds = gather(data['gt_preds']), gather(data['has_preds'])\n",
    "\n",
    "\n",
    "    reg = torch.cat([x for x in reg], 0)\n",
    "    gt_preds = torch.cat([x for x in gt_preds], 0)\n",
    "    has_preds = torch.cat([x for x in has_preds], 0)\n",
    "\n",
    "    reg = reg[:,:,:num_pred,:]\n",
    "    gt_preds = gt_preds[:,:num_pred,:]\n",
    "    has_preds = has_preds[:,:num_pred]\n",
    "\n",
    "    last = has_preds.float() + 0.1 * torch.arange(num_pred).float().to(\n",
    "                has_preds.device\n",
    "            ) / float(num_pred)\n",
    "\n",
    "    max_last, last_idcs = last.max(1)\n",
    "    mask = max_last >1.0\n",
    "\n",
    "    reg = reg[mask]\n",
    "    gt_preds = gt_preds[mask][:,:,:2]\n",
    "    has_preds = has_preds[mask]\n",
    "    last_idcs = last_idcs[mask]\n",
    "\n",
    "    row_idcs = torch.arange(len(last_idcs)).long().to(last_idcs.device)\n",
    "\n",
    "    return reg, gt_preds, has_preds, last_idcs, row_idcs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([151, 6, 80, 2]),\n",
       " torch.Size([151, 80, 2]),\n",
       " torch.Size([151, 80]),\n",
       " tensor([49, 79, 79, 79, 49, 79, 79, 79, 79,  1, 22, 79, 53,  5, 11, 21, 22, 10,\n",
       "         79, 39, 29, 79, 79, 24, 37, 41,  1, 13, 23, 79, 79, 30, 79, 35, 78,  1,\n",
       "         79, 51, 79, 74, 15,  3, 79, 79, 16, 79, 26, 79, 79, 79, 12, 17, 79, 79,\n",
       "         79, 79, 44, 10,  5, 18, 14, 79, 23, 79, 79, 79, 43, 79, 79, 79, 79, 79,\n",
       "         79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 60, 79, 79, 79, 79, 79, 67,\n",
       "         79, 79, 79, 38, 79, 79,  8, 24, 79, 79, 11, 79, 38, 79, 79, 79, 79, 79,\n",
       "         79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 74, 79, 79, 79, 79, 41,\n",
       "         79, 79, 79, 79, 70, 28, 79, 79, 79, 79, 79, 79, 68,  1, 79, 79, 36, 79,\n",
       "         55, 79, 14, 27, 14, 26, 79]),\n",
       " tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg, gt_preds, has_preds, last_idcs, row_idcs = get_lastIdcs(out,80,data)\n",
    "reg.shape, gt_preds.shape, has_preds.shape, last_idcs, row_idcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls, reg = out[\"cls\"], out[\"reg\"]\n",
    "gt_preds, has_preds = gather(data['gt_preds']), gather(data['has_preds'])\n",
    "\n",
    "reg = torch.cat([x for x in reg], 0)\n",
    "gt_preds = torch.cat([x for x in gt_preds], 0)\n",
    "has_preds = torch.cat([x for x in has_preds], 0)\n",
    "\n",
    "last = has_preds.float() + 0.1 * torch.arange(config[\"num_preds\"]).float().to(\n",
    "            has_preds.device\n",
    "        ) / float(config[\"num_preds\"])\n",
    "\n",
    "max_last, last_idcs = last.max(1)\n",
    "mask = max_last >1.0\n",
    "\n",
    "reg = reg[mask]\n",
    "gt_preds = gt_preds[mask][:,:,:2]\n",
    "has_preds = has_preds[mask]\n",
    "last_idcs = last_idcs[mask]\n",
    "\n",
    "row_idcs = torch.arange(len(last_idcs)).long().to(last_idcs.device)\n",
    "\n",
    "reg.shape, gt_preds.shape, has_preds.shape, last_idcs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minFDE(post_out,num_preds,data):\n",
    "    #num_preds = np.array([30, 50, 80])\n",
    "    minFDE = []\n",
    "    for j in range(len(num_preds)):\n",
    "        reg,gt_preds,_,last_idcs, row_idcs = get_lastIdcs(post_out, num_preds[j], data)\n",
    "\n",
    "        dist_6m = []\n",
    "        for i in range(config[\"num_mods\"]):\n",
    "            rr = reg[row_idcs,i,last_idcs]\n",
    "            gg = gt_preds[row_idcs,last_idcs].cuda()\n",
    "            dist = torch.sqrt(((rr - gg)**2).sum(1))\n",
    "            dist_6m.append(torch.tensor(dist).view(-1,1))\n",
    "\n",
    "        zz = torch.cat(dist_6m,1)\n",
    "        min_dist, min_idcs = zz.min(1)\n",
    "        fde = min_dist.mean().item()\n",
    "        minFDE.append(fde)\n",
    "    \n",
    "    mean = torch.tensor(minFDE).mean().item()\n",
    "    minFDE.append(mean)\n",
    "\n",
    "    return minFDE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_304061/3697859581.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dist_6m.append(torch.tensor(dist).view(-1,1))\n"
     ]
    }
   ],
   "source": [
    "minFDE = get_minFDE(out,config[\"metrics\"], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13.853604819102994, 20.4418553059963, 29.270918644413626, 21.188791275024414]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minFDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fde\n",
    "\n",
    "dist_6m = []\n",
    "for i in range(config[\"num_mods\"]):\n",
    "\n",
    "    rr = reg[row_idcs,i,last_idcs]\n",
    "    gg = gt_preds[row_idcs,last_idcs].cuda()\n",
    "    dist = torch.sqrt(((rr - gg)**2).sum(1))\n",
    "    dist_6m.append(torch.tensor(dist).view(-1,1))\n",
    "\n",
    "zz = torch.cat(dist_6m,1)\n",
    "min_dist, min_idcs = zz.min(1)\n",
    "\n",
    "fde = min_dist.mean().item()\n",
    "fde\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "#         dd = torch.sqrt(((rr[last_idcs] - gg[last_idcs])**2).sum(1))\n",
    "#         dist.append(dd.mean().item())\n",
    "    \n",
    "#     dist_6m.append(torch.tensor(dist).view(-1,1))\n",
    "\n",
    "# zz = torch.cat(dist_6m,1)\n",
    "# min_dist, min_idcs = zz.min(1)\n",
    "# # mask = ~torch.isnan(min_dist)\n",
    "# # mask\n",
    "# ade = min_dist.mean().item()\n",
    "# ade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minADE(post_out,num_preds,data):\n",
    "    #num_preds = np.array([30, 50, 80])\n",
    "    minADE = []\n",
    "    for j in range(len(num_preds)):\n",
    "        reg,gt_preds,has_preds,last_idcs, row_idcs = get_lastIdcs(post_out, num_preds[j], data)\n",
    "\n",
    "        dist_6m = []\n",
    "        for i in range(config[\"num_mods\"]):\n",
    "            \n",
    "            dist = []\n",
    "            for j in range(len(reg)):\n",
    "                rr = reg[j][i]\n",
    "                gg = gt_preds[j].cuda()\n",
    "                hh = has_preds[j].cuda()\n",
    "                dd = torch.sqrt(((rr[hh] - gg[hh])**2).sum(1))\n",
    "                dist.append(dd.mean().item())\n",
    "            \n",
    "            dist_6m.append(torch.tensor(dist).view(-1,1))\n",
    "\n",
    "        zz = torch.cat(dist_6m,1)\n",
    "        min_dist, min_idcs = zz.min(1)\n",
    "        ade = min_dist.mean().item()\n",
    "        minADE.append(ade)\n",
    "\n",
    "    mean = torch.tensor(minADE).mean().item()\n",
    "    minADE.append(mean)\n",
    "    \n",
    "    return minADE\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.310611724853516, 11.527812957763672, 15.718441009521484, 11.852288246154785]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minADE = get_minADE(out,config[\"metrics\"], data)\n",
    "minADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ade\n",
    "dist_6m = []\n",
    "for i in range(config[\"num_mods\"]):\n",
    "    reg_m = reg[:,i]\n",
    "    \n",
    "    dist = []\n",
    "    for j in range(len(reg_m)):\n",
    "    \n",
    "        rr = reg_m[j]\n",
    "        gg = gt_preds[j].cuda()\n",
    "        hh = has_preds[j].cuda()\n",
    "\n",
    "        dd = torch.sqrt(((rr[hh] - gg[hh])**2).sum(1))\n",
    "        dist.append(dd.mean().item())\n",
    "    \n",
    "    dist_6m.append(torch.tensor(dist).view(-1,1))\n",
    "\n",
    "zz = torch.cat(dist_6m,1)\n",
    "min_dist, min_idcs = zz.min(1)\n",
    "# mask = ~torch.isnan(min_dist)\n",
    "# mask\n",
    "ade = min_dist.mean().item()\n",
    "ade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = torch.cat([x for x in reg], 0)\n",
    "gt_preds = torch.cat([x for x in gt_preds], 0)\n",
    "has_preds = torch.cat([x for x in has_preds], 0)\n",
    "\n",
    "reg.shape, gt_preds.shape, has_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = has_preds.float() + 0.1 * torch.arange(80).float().to(\n",
    "            has_preds.device\n",
    "        ) / float(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_last, last_idcs = last.max(1)\n",
    "max_last, last_idcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = max_last >1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_preds[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_idcs = torch.arange(len(last_idcs[mask])).long().to(last_idcs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_idcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = reg[:,0]\n",
    "first.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = first[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = has_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = []\n",
    "for i in range(len(first)):\n",
    "    a = first[i]\n",
    "    b = has_preds[i]\n",
    "    c = a[b].sum(1)\n",
    "    dist.append(c.mean().item())\n",
    "\n",
    "len(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dist_6m = []\n",
    "for i in range(6):\n",
    "    reg_m = reg[:,i]\n",
    "    has_m = has_preds[:,i]\n",
    "\n",
    "    dist = []\n",
    "    for j in range(len(reg_m)):\n",
    "    \n",
    "        rr = reg_m[j]\n",
    "        gg = gt_preds[j][:,:2].cuda()\n",
    "        hh = has_m[j].cuda()\n",
    "\n",
    "        dd = torch.sqrt(((rr[hh] - gg[hh])**2).sum(1))\n",
    "        dist.append(dd.mean().item())\n",
    "    \n",
    "    dist_6m.append(torch.tensor(dist).view(-1,1))\n",
    "\n",
    "zz = torch.cat(dist_6m,1)\n",
    "min_dist, min_idcs = zz.min(1)\n",
    "mask = ~torch.isnan(min_dist)\n",
    "mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg[row_idcs,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_idcs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg[row_idcs,0,last_idcs[mask]].sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_out = post_process(out, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_process.append(metrics, loss_out, post_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(metrics[\"preds\"], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['gt_preds'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gather(data[\"gt_preds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['reg'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[out[\"reg\"][i].size() for i in range(len(out['reg']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_out = dict()\n",
    "post_out[\"preds\"] = [x[0:1].detach().cpu().numpy() for x in out[\"reg\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['cls'][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['reg'][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(out[\"reg\"])):\n",
    "    1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = torch.ones(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.view(1,1,1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
